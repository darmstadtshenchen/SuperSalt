{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ec6c04-42c5-4645-8e60-4e902cfd141e",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56e383-48f3-4181-abc5-66ca50a19cb0",
   "metadata": {},
   "source": [
    "For BO environment  \n",
    "- pip install -r requirement.txt\n",
    "\n",
    "For magpie descriptor  \n",
    "- install magpie java version: https://wolverton.bitbucket.io/tutorial.html\n",
    "- put magpie-latest folder in the same folder of src\n",
    "- mv compound.in into magpie-latest folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f3ce6-a8e5-4a14-8b6a-33cedcfeca98",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9c964-6104-43b6-9c8a-0be55ecd903a",
   "metadata": {},
   "source": [
    "## Composition information preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97454fd-64a3-42f1-bac1-8a4b12204767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, shutil, json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea06da0-f38f-4914-a4d8-5acd35e44f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_atoms(molecule):\n",
    "    split_molecule = re.findall(r'[A-Z][a-z]?(?:\\d+\\.\\d+|\\d+(?:\\.\\d+)?)?', molecule)\n",
    "    #print(split_molecule)\n",
    "    result = dict()\n",
    "    eles = []\n",
    "    for split in split_molecule:\n",
    "        m = re.match('([A-Z][a-z]?)(\\d+(\\.\\d*)?)?', split)\n",
    "        element = m.group(1)\n",
    "        count = float(m.group(2)) if m.group(2) else 1\n",
    "        eles.append(element)\n",
    "        #result.append([element, str(count)])\n",
    "        result[element] = count\n",
    "    return [eles,result]\n",
    "\n",
    "def write_magpie(select_set, proplist, trianset=True):\n",
    "    if trianset:\n",
    "        file_name = 'chen_composition_train.txt'\n",
    "    else:\n",
    "        file_name = 'chen_composition_pred.txt'\n",
    "    with open(f'./Dataprocessing/{file_name}', 'w') as f:\n",
    "        f.write(f'name   {\"   \".join([prop for prop in proplist])}\\n')\n",
    "        for num in range(len(select_set)):\n",
    "            # print(select_set['Composition'].iloc[num])\n",
    "            ele, res = separate_atoms(select_set['composition'].iloc[num])\n",
    "            for j in ele:\n",
    "                f.write(f'{j},{res[j]},')\n",
    "            if trianset:\n",
    "                f.write(f'    {\"    \".join([str(list(select_set[prop])[num] if prop in select_set.columns else \"None\") for prop in proplist ])}\\n')\n",
    "            else:\n",
    "                f.write(f'    {\"    \".join([\"None\" for prop in proplist])}\\n')\n",
    "\n",
    "def magpie_to_csv(root, sets, fileN='chen_composition'):\n",
    "\n",
    "    for train in sets:\n",
    "        if train:\n",
    "            file_name = f'{fileN}_train'\n",
    "        else:\n",
    "            file_name = f'{fileN}_pred'\n",
    "            \n",
    "        if os.path.exists(f'{root}/Dataprocessing/{file_name}.txt'):\n",
    "            shutil.copyfile(f'{root}/Dataprocessing/{file_name}.txt', f'{root}/Dataprocessing/processing_data.txt')\n",
    "            os.chdir('./magpie-latest')\n",
    "            os.system('java -jar dist/Magpie.jar compound.in')\n",
    "            shutil.copyfile(f'{root}/Dataprocessing/processing_data.json', f'{root}/Dataprocessing/{file_name}.json')\n",
    "            os.chdir(root)\n",
    "    \n",
    "    ### json data --> csv\n",
    "    \n",
    "    for train in sets:\n",
    "        if train:\n",
    "            file_name = f'{fileN}_train'\n",
    "        else:\n",
    "            file_name = f'{fileN}_pred'\n",
    "    \n",
    "        if os.path.exists(f'{root}/Dataprocessing/{file_name}.json'):\n",
    "        \n",
    "            with open(f'{root}/Dataprocessing/{file_name}.json') as f:\n",
    "               jsdata = json.load(f)\n",
    "        \n",
    "            all_info = []\n",
    "            for i in jsdata['entries']:\n",
    "                if train:\n",
    "                    info = [i['composition']]+i['attributes']+[i['properties'][num]['measured'] if 'measured' in i['properties'][num].keys() else \"None\" for num in range(len(i['properties']))]\n",
    "                else:\n",
    "                    info = [i['composition']]+i['attributes']+['None'  for num in range(len(i['properties']))]\n",
    "                all_info.append(info)\n",
    "            print(all_info[0])\n",
    "            print(len(all_info))\n",
    "            \n",
    "            infodf = pd.DataFrame(all_info, columns = ['composition']+jsdata['attribute-names']+[jsdata['properties'][num]['name'] for  num in range(len(jsdata['properties']))])\n",
    "            infodf.to_csv(f'{root}/Dataprocessing/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a1d56-a25e-48c2-a6ea-0fee5de2a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### file should contain at least a column of composition information and a column of properties. If for candidate data, no property is available, each line in the column can be filled with 'None'.\n",
    "\n",
    "### specify the data path and name of properties\n",
    "root = os.getcwd()\n",
    "database_name = os.getcwd()+'/Dataprocessing/chen_data.csv'\n",
    "alldata = pd.read_csv(database_name) #, sep='\\s+')\n",
    "_prop = ['formation_enthalpy(eV/atom)']\n",
    "os.chdir(root)\n",
    "\n",
    "### write magpie file for train and predict (if exist) set\n",
    "predict_set = alldata[alldata[_prop].isnull().any(axis=1)]\n",
    "if not predict_set.empty:\n",
    "    write_magpie(predict_set, _prop, trianset=False)\n",
    "    sets = [True, False]\n",
    "else:\n",
    "    sets = [True]\n",
    "    \n",
    "train_set = alldata[alldata[_prop].notnull().all(axis=1)]\n",
    "\n",
    "write_magpie(train_set, _prop, trianset=True)\n",
    "\n",
    "magpie_to_csv(root, sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04cfea-5712-4242-a5b7-bfddbbe7570f",
   "metadata": {},
   "source": [
    "## Candidates generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d72bd6-226b-4a7f-bbbb-6de6801d6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "### Candidate ratio generation\n",
    "step_size = 0.05\n",
    "values = np.arange(0, 1 + step_size, step_size)\n",
    "num_elements = 11\n",
    "\n",
    "def find_combinations(num_elements, target_sum, current_combination, results):\n",
    "    if num_elements == 1:\n",
    "        if np.isclose(target_sum, round(target_sum, 2)) and target_sum >= 0 and target_sum <= 1:\n",
    "            results.append(current_combination + [round(target_sum, 2)])\n",
    "        return\n",
    "\n",
    "    for value in values:\n",
    "        if value > target_sum:\n",
    "            break\n",
    "        find_combinations(num_elements - 1, target_sum - value, current_combination + [value], results)\n",
    "\n",
    "results = []\n",
    "find_combinations(num_elements, 1.0, [], results)\n",
    "\n",
    "with open('combinations_11.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0d0ab-b24d-4d5f-9e2a-e43b0ef9f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(n, total, combination):\n",
    "    if n == 1:\n",
    "        yield combination + [total]\n",
    "    else:\n",
    "        for i in range(total + 1):\n",
    "            yield from generate_combinations(n - 1, total - i, combination + [i])\n",
    "\n",
    "combinations_list = list(generate_combinations(11, 20, []))\n",
    "\n",
    "print(len(combinations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d489d-bd69-40e7-864d-cc40e560e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the files for Magpie of candidates\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "\n",
    "#[LiCl, NaCl, KCl, RbCl, CsCl, MgCl2, CaCl2, SrCl2, BaCl2, ZnCl2, ZrCl4]\n",
    "comp_list = ['Li', 'Na', 'K', 'Rb', 'Cs', 'Mg', 'Ca', 'Sr', 'Ba', 'Zn', 'Zr']\n",
    "coord_list = np.array([1,1,1,1,1,2,2,2,2,2,4])\n",
    "\n",
    "### set the temp to be 1000\n",
    "def write_data(npdf, name, comp_list=comp_list, coord_list=coord_list):\n",
    "    with open(name+'.txt', 'w') as f:\n",
    "        f.write('name   Temp   denstiy\\n')\n",
    "        for i in npdf:\n",
    "            f.write(''.join([f'{comp_list[num]},{round(i[num],3)},' for num in range(len(comp_list))]))\n",
    "            f.write(f\"Cl,{round(np.sum(np.array(i)*coord_list),3)},   \")\n",
    "            f.write(f\"1000   None\\n\")\n",
    "    with open('finish.txt', 'w') as f:\n",
    "        f.write('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1adbcd-6d22-4fca-b85e-4afe097eb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combinations_11.pickle', 'rb') as f:\n",
    "    combines = pickle.load(f)\n",
    "\n",
    "write_data(combines, 'chen_magpie_candidates_train')\n",
    "sets = [True]\n",
    "magpie_to_csv(root, sets=sets, fileN='chen_magpie_candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41499bc-c905-45b9-a8c4-018d0751e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Candidate separation code for those very large candidate file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('./Dataprocessing/chen_magpie_candidates_train.csv')\n",
    "\n",
    "sepreate_num = 20\n",
    "sep_grid = int(len(data)/sepreate_num)+1\n",
    "for i in range(20):\n",
    "    next = min((i+1)*sep_grid, len(data))\n",
    "    new_data = data[i*sep_grid:next]\n",
    "    new_data.to_csv(f'{os.getcwd()}/sep_cand_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724de10-21b2-4c45-bccd-25fb34eec1e4",
   "metadata": {},
   "source": [
    "Then move the data from Dataprocessing to Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dcb52-3553-4d6d-a7b0-d1a72485d634",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Closed pool test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea93c84d-12b1-42fd-af5d-61eb3e98ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/mnt/Database/Yixuan/TMM_BO_project_Chen/performance_record.txt': No such file or directory\n",
      "rm: cannot remove '/mnt/Database/Yixuan/TMM_BO_project_Chen/model_weights': No such file or directory\n",
      "2024-11-26 14:28:28,933\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from src.bayesian_optimization import BayesianOptimization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root = os.getcwd()\n",
    "os.popen(f'rm -r {root}/performance_record.txt {root}/model_weights')\n",
    "\n",
    "target_props = ['B0', 'Ms']\n",
    "feature_props = ['NComp', 'Comp_L3Norm', 'Comp_L5Norm', 'Comp_L7Norm', 'Comp_L10Norm', 'mean_Number', 'maxdiff_Number', 'dev_Number', 'max_Number', 'min_Number', 'most_Number', 'mean_MendeleevNumber', 'maxdiff_MendeleevNumber', 'dev_MendeleevNumber', 'max_MendeleevNumber', 'min_MendeleevNumber', 'most_MendeleevNumber', 'mean_AtomicWeight', 'maxdiff_AtomicWeight', 'dev_AtomicWeight', 'max_AtomicWeight', 'min_AtomicWeight', 'most_AtomicWeight', 'mean_MeltingT', 'maxdiff_MeltingT', 'dev_MeltingT', 'max_MeltingT', 'min_MeltingT', 'most_MeltingT', 'mean_Column', 'maxdiff_Column', 'dev_Column', 'max_Column', 'min_Column', 'most_Column', 'mean_Row', 'maxdiff_Row', 'dev_Row', 'max_Row', 'min_Row', 'most_Row', 'mean_CovalentRadius', 'maxdiff_CovalentRadius', 'dev_CovalentRadius', 'max_CovalentRadius', 'min_CovalentRadius', 'most_CovalentRadius', 'mean_Electronegativity', 'maxdiff_Electronegativity', 'dev_Electronegativity', 'max_Electronegativity', 'min_Electronegativity', 'most_Electronegativity', 'mean_NsValence', 'maxdiff_NsValence', 'dev_NsValence', 'max_NsValence', 'min_NsValence', 'most_NsValence', 'mean_NpValence', 'maxdiff_NpValence', 'dev_NpValence', 'max_NpValence', 'min_NpValence', 'most_NpValence', 'mean_NdValence', 'maxdiff_NdValence', 'dev_NdValence', 'max_NdValence', 'min_NdValence', 'most_NdValence', 'mean_NfValence', 'maxdiff_NfValence', 'dev_NfValence', 'max_NfValence', 'min_NfValence', 'most_NfValence', 'mean_NValance', 'maxdiff_NValance', 'dev_NValance', 'max_NValance', 'min_NValance', 'most_NValance', 'mean_NsUnfilled', 'maxdiff_NsUnfilled', 'dev_NsUnfilled', 'max_NsUnfilled', 'min_NsUnfilled', 'most_NsUnfilled', 'mean_NpUnfilled', 'maxdiff_NpUnfilled', 'dev_NpUnfilled', 'max_NpUnfilled', 'min_NpUnfilled', 'most_NpUnfilled', 'mean_NdUnfilled', 'maxdiff_NdUnfilled', 'dev_NdUnfilled', 'max_NdUnfilled', 'min_NdUnfilled', 'most_NdUnfilled', 'mean_NfUnfilled', 'maxdiff_NfUnfilled', 'dev_NfUnfilled', 'max_NfUnfilled', 'min_NfUnfilled', 'most_NfUnfilled', 'mean_NUnfilled', 'maxdiff_NUnfilled', 'dev_NUnfilled', 'max_NUnfilled', 'min_NUnfilled', 'most_NUnfilled', 'mean_GSvolume_pa', 'maxdiff_GSvolume_pa', 'dev_GSvolume_pa', 'max_GSvolume_pa', 'min_GSvolume_pa', 'most_GSvolume_pa', 'mean_GSbandgap', 'maxdiff_GSbandgap', 'dev_GSbandgap', 'max_GSbandgap', 'min_GSbandgap', 'most_GSbandgap', 'mean_GSmagmom', 'maxdiff_GSmagmom', 'dev_GSmagmom', 'max_GSmagmom', 'min_GSmagmom', 'most_GSmagmom', 'mean_SpaceGroupNumber', 'maxdiff_SpaceGroupNumber', 'dev_SpaceGroupNumber', 'max_SpaceGroupNumber', 'min_SpaceGroupNumber', 'most_SpaceGroupNumber', 'frac_sValence', 'frac_pValence', 'frac_dValence', 'frac_fValence', 'CanFormIonic', 'MaxIonicChar', 'MeanIonicChar']\n",
    "# data_file = 'Data/chen_comp_train_20.csv'\n",
    "data_file = 'Data/quat_4magpie.csv'\n",
    "\n",
    "# available models: \n",
    "# ['Ridge', 'Lasso', 'ElasticNet', 'KNeighborsRegressor', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor', 'GradientBoostingRegressor', 'AdaBoostRegressor', 'ExtraTreesRegressor', 'XGBoost', 'LightGBM', 'GaussianProcess', 'FastKAN'] \n",
    "\n",
    "model_list = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor']\n",
    "candi_data_file = 'Data/quat_4magpie.csv'\n",
    "BO = BayesianOptimization(target_props, data_file=data_file, feature_props=feature_props, model_list=model_list, acq_method='ucb', stacking=True, close_pool=True)#, select_region=[[2.200],[2.201]])\n",
    "# BO.close_pooling_test(n_bootstrap_sample_nums=20, n_iter=100, batch_size=10, hpar=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2768fb-ad88-4b5d-9338-b0dbd166e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    # BO.close_pooling_test(n_bootstrap_sample_nums=10, n_iter=100, batch_size=10, hpar=0.2)\n",
    "    \n",
    "    file_path = f'{root}/performance_record.txt'\n",
    "    data = pd.read_csv(file_path, sep=r'\\t', engine='python')\n",
    "    \n",
    "    # 绘制图形\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # 绘制 current_max_value 折线\n",
    "    ax.plot(data['number_of_samples'], data['current_best_value'], label='Max Value in train set', marker='o', lw=3, markersize=10)\n",
    "    \n",
    "    ax.plot(data['number_of_samples'], data['best_value_of_this_iteration'], label='Max value found in each iteration', marker='o', lw=3, markersize=10)\n",
    "    \n",
    "    # 绘制 mean_value_of_this_iteration 折线及其置信区间\n",
    "    ax.errorbar(data['number_of_samples'], data['mean_value_of_this_iteration'],\n",
    "                yerr=data['std_of_this_iteration'], label='Mean Value of each iteration with Confidence Interval', marker='o', lw=3, capsize=5, markersize=10)\n",
    "    \n",
    "    # 添加标签和标题\n",
    "    ax.set_xlabel('Number of Samples', size=20)\n",
    "    ax.set_ylabel('Target value', size=20)\n",
    "    ax.set_title('Algorithm Performance Over Iterations', size=20)\n",
    "    ax.legend(prop=dict(size=14))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    #ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%d'))\n",
    "    #ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{root}/performance_record.png', bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    os.popen(f'mv model_weights model_weights_{i}')\n",
    "    os.popen(f'mv performance_record* model_weights_{i}')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc84b70-224a-4a17-8a7b-f8db8ae94383",
   "metadata": {},
   "source": [
    "# Training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ad39c-a76d-4478-a43b-6976d18c11f2",
   "metadata": {},
   "source": [
    "## Calling TMMBO, training, loading model and predicting candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f032fe53-5d08-4a0e-9f38-6c3c010ba47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop feature which is non numeric: composition\n",
      "length of cleaned data: 400\n",
      "used feature set: ['NComp', 'Comp_L2Norm', 'Comp_L3Norm', 'Comp_L5Norm', 'Comp_L7Norm', 'Comp_L10Norm', 'mean_Number', 'maxdiff_Number', 'dev_Number', 'max_Number', 'min_Number', 'most_Number', 'mean_MendeleevNumber', 'maxdiff_MendeleevNumber', 'dev_MendeleevNumber', 'max_MendeleevNumber', 'min_MendeleevNumber', 'most_MendeleevNumber', 'mean_AtomicWeight', 'maxdiff_AtomicWeight', 'dev_AtomicWeight', 'max_AtomicWeight', 'min_AtomicWeight', 'most_AtomicWeight', 'mean_MeltingT', 'maxdiff_MeltingT', 'dev_MeltingT', 'max_MeltingT', 'min_MeltingT', 'most_MeltingT', 'mean_Column', 'maxdiff_Column', 'dev_Column', 'max_Column', 'min_Column', 'most_Column', 'mean_Row', 'maxdiff_Row', 'dev_Row', 'max_Row', 'min_Row', 'most_Row', 'mean_CovalentRadius', 'maxdiff_CovalentRadius', 'dev_CovalentRadius', 'max_CovalentRadius', 'min_CovalentRadius', 'most_CovalentRadius', 'mean_Electronegativity', 'maxdiff_Electronegativity', 'dev_Electronegativity', 'max_Electronegativity', 'min_Electronegativity', 'most_Electronegativity', 'mean_NsValence', 'maxdiff_NsValence', 'dev_NsValence', 'max_NsValence', 'min_NsValence', 'most_NsValence', 'mean_NpValence', 'maxdiff_NpValence', 'dev_NpValence', 'max_NpValence', 'min_NpValence', 'most_NpValence', 'mean_NdValence', 'maxdiff_NdValence', 'dev_NdValence', 'max_NdValence', 'min_NdValence', 'most_NdValence', 'mean_NfValence', 'maxdiff_NfValence', 'dev_NfValence', 'max_NfValence', 'min_NfValence', 'most_NfValence', 'mean_NValance', 'maxdiff_NValance', 'dev_NValance', 'max_NValance', 'min_NValance', 'most_NValance', 'mean_NsUnfilled', 'maxdiff_NsUnfilled', 'dev_NsUnfilled', 'max_NsUnfilled', 'min_NsUnfilled', 'most_NsUnfilled', 'mean_NpUnfilled', 'maxdiff_NpUnfilled', 'dev_NpUnfilled', 'max_NpUnfilled', 'min_NpUnfilled', 'most_NpUnfilled', 'mean_NdUnfilled', 'maxdiff_NdUnfilled', 'dev_NdUnfilled', 'max_NdUnfilled', 'min_NdUnfilled', 'most_NdUnfilled', 'mean_NfUnfilled', 'maxdiff_NfUnfilled', 'dev_NfUnfilled', 'max_NfUnfilled', 'min_NfUnfilled', 'most_NfUnfilled', 'mean_NUnfilled', 'maxdiff_NUnfilled', 'dev_NUnfilled', 'max_NUnfilled', 'min_NUnfilled', 'most_NUnfilled', 'mean_GSvolume_pa', 'maxdiff_GSvolume_pa', 'dev_GSvolume_pa', 'max_GSvolume_pa', 'min_GSvolume_pa', 'most_GSvolume_pa', 'mean_GSbandgap', 'maxdiff_GSbandgap', 'dev_GSbandgap', 'max_GSbandgap', 'min_GSbandgap', 'most_GSbandgap', 'mean_GSmagmom', 'maxdiff_GSmagmom', 'dev_GSmagmom', 'max_GSmagmom', 'min_GSmagmom', 'most_GSmagmom', 'mean_SpaceGroupNumber', 'maxdiff_SpaceGroupNumber', 'dev_SpaceGroupNumber', 'max_SpaceGroupNumber', 'min_SpaceGroupNumber', 'most_SpaceGroupNumber', 'frac_sValence', 'frac_pValence', 'frac_dValence', 'frac_fValence', 'CanFormIonic', 'MaxIonicChar', 'MeanIonicChar', 'Temp']\n",
      "drop feature which is non numeric: composition\n",
      "length of cleaned data: 400\n",
      "used feature set: ['NComp', 'Comp_L2Norm', 'Comp_L3Norm', 'Comp_L5Norm', 'Comp_L7Norm', 'Comp_L10Norm', 'mean_Number', 'maxdiff_Number', 'dev_Number', 'max_Number', 'min_Number', 'most_Number', 'mean_MendeleevNumber', 'maxdiff_MendeleevNumber', 'dev_MendeleevNumber', 'max_MendeleevNumber', 'min_MendeleevNumber', 'most_MendeleevNumber', 'mean_AtomicWeight', 'maxdiff_AtomicWeight', 'dev_AtomicWeight', 'max_AtomicWeight', 'min_AtomicWeight', 'most_AtomicWeight', 'mean_MeltingT', 'maxdiff_MeltingT', 'dev_MeltingT', 'max_MeltingT', 'min_MeltingT', 'most_MeltingT', 'mean_Column', 'maxdiff_Column', 'dev_Column', 'max_Column', 'min_Column', 'most_Column', 'mean_Row', 'maxdiff_Row', 'dev_Row', 'max_Row', 'min_Row', 'most_Row', 'mean_CovalentRadius', 'maxdiff_CovalentRadius', 'dev_CovalentRadius', 'max_CovalentRadius', 'min_CovalentRadius', 'most_CovalentRadius', 'mean_Electronegativity', 'maxdiff_Electronegativity', 'dev_Electronegativity', 'max_Electronegativity', 'min_Electronegativity', 'most_Electronegativity', 'mean_NsValence', 'maxdiff_NsValence', 'dev_NsValence', 'max_NsValence', 'min_NsValence', 'most_NsValence', 'mean_NpValence', 'maxdiff_NpValence', 'dev_NpValence', 'max_NpValence', 'min_NpValence', 'most_NpValence', 'mean_NdValence', 'maxdiff_NdValence', 'dev_NdValence', 'max_NdValence', 'min_NdValence', 'most_NdValence', 'mean_NfValence', 'maxdiff_NfValence', 'dev_NfValence', 'max_NfValence', 'min_NfValence', 'most_NfValence', 'mean_NValance', 'maxdiff_NValance', 'dev_NValance', 'max_NValance', 'min_NValance', 'most_NValance', 'mean_NsUnfilled', 'maxdiff_NsUnfilled', 'dev_NsUnfilled', 'max_NsUnfilled', 'min_NsUnfilled', 'most_NsUnfilled', 'mean_NpUnfilled', 'maxdiff_NpUnfilled', 'dev_NpUnfilled', 'max_NpUnfilled', 'min_NpUnfilled', 'most_NpUnfilled', 'mean_NdUnfilled', 'maxdiff_NdUnfilled', 'dev_NdUnfilled', 'max_NdUnfilled', 'min_NdUnfilled', 'most_NdUnfilled', 'mean_NfUnfilled', 'maxdiff_NfUnfilled', 'dev_NfUnfilled', 'max_NfUnfilled', 'min_NfUnfilled', 'most_NfUnfilled', 'mean_NUnfilled', 'maxdiff_NUnfilled', 'dev_NUnfilled', 'max_NUnfilled', 'min_NUnfilled', 'most_NUnfilled', 'mean_GSvolume_pa', 'maxdiff_GSvolume_pa', 'dev_GSvolume_pa', 'max_GSvolume_pa', 'min_GSvolume_pa', 'most_GSvolume_pa', 'mean_GSbandgap', 'maxdiff_GSbandgap', 'dev_GSbandgap', 'max_GSbandgap', 'min_GSbandgap', 'most_GSbandgap', 'mean_GSmagmom', 'maxdiff_GSmagmom', 'dev_GSmagmom', 'max_GSmagmom', 'min_GSmagmom', 'most_GSmagmom', 'mean_SpaceGroupNumber', 'maxdiff_SpaceGroupNumber', 'dev_SpaceGroupNumber', 'max_SpaceGroupNumber', 'min_SpaceGroupNumber', 'most_SpaceGroupNumber', 'frac_sValence', 'frac_pValence', 'frac_dValence', 'frac_fValence', 'CanFormIonic', 'MaxIonicChar', 'MeanIonicChar', 'Temp']\n",
      "Initialisation\n",
      "Fitting\n",
      "{'alpha': 1.0985109161419334e-05}\n",
      "{'alpha': 1.0820095607164685e-05, 'l1_ratio': 0.015268900213234342}\n",
      "Candidate selection\n",
      "start Lasso\n",
      "start ElasticNet\n",
      "start stacking model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2.00000000e+00, 7.45355992e-01, 6.93361274e-01, ...,\n",
       "         7.24240177e-01, 3.21884523e-01, 1.25000000e+03],\n",
       "        [3.00000000e+00, 7.07815297e-01, 6.62063797e-01, ...,\n",
       "         7.24240177e-01, 3.31226139e-01, 1.00000000e+03],\n",
       "        [3.00000000e+00, 7.18688679e-01, 6.71639139e-01, ...,\n",
       "         7.45613369e-01, 3.30599916e-01, 1.00000000e+03],\n",
       "        ...,\n",
       "        [4.00000000e+00, 7.00749920e-01, 6.72338177e-01, ...,\n",
       "         7.24240177e-01, 2.93058729e-01, 1.10000000e+03],\n",
       "        [2.00000000e+00, 7.45355992e-01, 6.93361274e-01, ...,\n",
       "         4.34488699e-01, 1.93106089e-01, 7.63000000e+02],\n",
       "        [3.00000000e+00, 6.84653197e-01, 6.39653743e-01, ...,\n",
       "         7.45613369e-01, 3.42903660e-01, 1.15000000e+03]]),\n",
       " array([395, 377, 378, 376, 398, 396, 387, 375, 399, 374]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from src.bayesian_optimization import BayesianOptimization\n",
    "from src.io import IOManager\n",
    "\n",
    "root = os.getcwd()\n",
    "\n",
    "target_props = ['density']\n",
    "data_file = 'Data/chen_comp_train.csv'\n",
    "# data_file = 'Data/chen_comp_region_train_10.csv'\n",
    "model_list = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor', 'GradientBoostingRegressor', 'ExtraTreesRegressor', 'XGBoost', 'LightGBM']#, 'GaussianProcess']\n",
    "candi_data_file = 'Data/chen_comp_train.csv'\n",
    "BO = BayesianOptimization(data_file, target_props, model_list=model_list, acq_method='ucb', candidate_file=candi_data_file)#, select_region=[[2.200],[2.201]])\n",
    "BO.optimize(batch_size=10, n_bootstrap_sample_nums=81, sampling_method='gaussian', num_candidate=100, n_samples=1000, iterations=5, hpar=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84488e-14ab-4d35-90e3-5fe92b496ff6",
   "metadata": {},
   "source": [
    "## Fitting results check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163c66a-34e9-4a36-ac30-b63e8cc89433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle, os\n",
    "from src.bayesian_optimization import BayesianOptimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "root = os.getcwd()\n",
    "target_props = ['density']\n",
    "data_file = 'Data/chen_comp_train.csv'\n",
    "# data_file = 'Data/chen_comp_train_22r6.csv'\n",
    "model_list = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor', 'GradientBoostingRegressor', 'ExtraTreesRegressor', 'XGBoost', 'LightGBM']\n",
    "model_list = ['Lasso', 'ElasticNet']\n",
    "candi_data_file = 'Data/chen_comp_train.csv'\n",
    "\n",
    "BO = BayesianOptimization(data_file, target_props, model_list=model_list, acq_method='ucb', candidate_file=candi_data_file)#, select_region=[[2.200],[2.201]])\n",
    "X_train, y_train = BO.io_manager.read_data(data_file, target_props=BO.target_props, feature_props=BO.feature_props, handle_null=True, drop_non_numeric=True)\n",
    "# X_candi = io_manager.read_candidate_data(candi_data_file, target_props=BO.target_props, feature_props=BO.feature_props, drop_non_numeric=True)\n",
    "# X_scaled, y_scaled, candidate_X_scaled = io_manager.standardize_data(X_train, y_train, X_candi)\n",
    "X_candi, y_candi = BO.io_manager.read_data(candi_data_file, target_props=BO.target_props, feature_props=BO.feature_props, handle_null=True, drop_non_numeric=True)\n",
    "X_scaled, y_scaled, candidate_X_scaled, candidate_y_scaled = BO.io_manager.standardize_data(X_train, y_train, X_candi, y_candi)\n",
    "# candidate_X_scaled, candidate_y_scaled = io_manager.standardize_data(X_candi, y_candi)\n",
    "# X_scaled, y_scaled = io_manager.standardize_data(X_train, y_train)\n",
    "\n",
    "model_path = f'model_weights'\n",
    "models = [i for i in os.listdir(f'{model_path}') if i.endswith('.pkl')]\n",
    "# models = [i for i in os.listdir(f'{model_path}') if i.endswith('.pkl') and i.split('0')[0][:-1] in model_list]\n",
    "\n",
    "for m in models:\n",
    "    with open(f'{model_path}/{m}', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    if 'bootstrap' in m:\n",
    "        bs_models = model['models']\n",
    "        pred_y = np.mean([m.predict(candidate_X_scaled) for m in bs_models], axis=0)\n",
    "    else:\n",
    "        bs_models = model['stacking_model']\n",
    "        pred_y = bs_models.predict(candidate_X_scaled)\n",
    "\n",
    "    # print(pred_y.reshape(-1,1))\n",
    "    # io_manager.scaler_y.fit(y_train)\n",
    "    y_prediction = BO.io_manager.scaler_y.inverse_transform(pred_y.reshape(-1,1))\n",
    "\n",
    "    # print(y_prediction)\n",
    "\n",
    "    r2 = r2_score(y_prediction, y_candi)\n",
    "    mse = mean_squared_error(y_prediction, y_candi)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(y_candi, y_prediction.reshape(-1))\n",
    "    miny, maxy = min(np.min(y_prediction), np.min(y_candi)), max(np.max(y_prediction), np.max(y_candi))\n",
    "    X_axis = np.linspace(miny, maxy, num=50)\n",
    "    ax.plot(X_axis, X_axis, c='r')\n",
    "    ax.set_title(f'Models {m}', size=20)\n",
    "    ax.text(miny+0.05, maxy-0.1, s=f\"MSE: {round(mse,3)}\", fontsize=18, color='black')\n",
    "    ax.text(miny+0.05, maxy-0.2, s=f\"R2: {round(r2,3)}\", fontsize=18, color='black')\n",
    "    plt.savefig(f'{model_path}/{m[:-4]}.png', bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    csv_file = pd.read_csv(candi_data_file)\n",
    "    csv_file['density'] = y_prediction\n",
    "    csv_file.to_csv(f'{model_path}/{m[:-4]}.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f85d4-c946-4dbd-a138-c216e6c72e21",
   "metadata": {},
   "source": [
    "## Candidates selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56edf4-a910-4816-af0d-469f84ac9c40",
   "metadata": {},
   "source": [
    "### Subset screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809bcae-31c8-4f96-8930-2b6ec97473af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.bayesian_optimization import BayesianOptimization\n",
    "from src.io import IOManager\n",
    "\n",
    "root = os.getcwd()\n",
    "# os.popen(f'rm -r {root}/model_weights')\n",
    "\n",
    "target_props = ['density']\n",
    "# data_file = 'Data/chen_comp_train_20.csv'\n",
    "data_file = 'Data/chen_comp_train_maxr3.csv'\n",
    "model_list = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor', 'GradientBoostingRegressor', 'ExtraTreesRegressor', 'XGBoost', 'LightGBM']#, 'GaussianProcess']\n",
    "\n",
    "for count in range(1,21):\n",
    "    candi_data_file = f'Chen_candidate/sep_candidate_{count}.csv'\n",
    "    # candi_data_file = f'Chen_candidate/filtered_compositions_{count}.csv'\n",
    "    \n",
    "    BO = BayesianOptimization(data_file, target_props, model_list=model_list, acq_method='ucb', candidate_file=candi_data_file)#, select_region=[[2.200],[2.201]])\n",
    "    # print(BO.model_list)\n",
    "    BO.optimize(batch_size=10000, n_bootstrap_sample_nums=81, sampling_method='gaussian', num_candidate=1000, n_samples=1000, iterations=5, hpar=0.1, if_train=False)  \n",
    "    os.popen(f'mv {root}/suggested_samples_original.csv {root}/Chen_candidate/suggested_samples_original_{count}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9416b-e37c-47b7-8dca-49b5550cb782",
   "metadata": {},
   "source": [
    "### merge screened out candidates set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34738b-8ed3-433d-aadd-246eb4e77d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# trainset_path = 'Data/chen_comp_train_20.csv'\n",
    "trainset_path = 'Data/chen_comp_train_maxr3.csv'\n",
    "train_df = pd.read_csv(trainset_path)\n",
    "\n",
    "feature_columns = train_df.columns.drop('density')\n",
    "# feature_columns = train_df.columns\n",
    "\n",
    "combined_test_df = pd.DataFrame()\n",
    "\n",
    "testset_folder = 'Chen_candidate'\n",
    "\n",
    "for file_name in os.listdir(testset_folder):\n",
    "    if file_name.endswith('.csv') and file_name.startswith('suggested_samples_original_'):\n",
    "        testset_path = os.path.join(testset_folder, file_name)\n",
    "        \n",
    "        test_df = pd.read_csv(testset_path)\n",
    "        \n",
    "        test_df.columns = feature_columns\n",
    "        \n",
    "        test_df['density'] = None\n",
    "        \n",
    "        combined_test_df = pd.concat([combined_test_df, test_df], ignore_index=True)\n",
    "\n",
    "combined_test_df.to_csv(f'Chen_candidate/combined_testset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dab7a-b548-442b-8010-c7b47e1886e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.bayesian_optimization import BayesianOptimization\n",
    "from src.io import IOManager\n",
    "\n",
    "root = os.getcwd()\n",
    "\n",
    "target_props = ['density']\n",
    "# data_file = 'Data/chen_comp_train.csv'\n",
    "data_file = 'Data/chen_comp_train_maxr3.csv'\n",
    "model_list = ['Ridge', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor', 'RandomForest', 'SVR', 'MLPRegressor', 'GradientBoostingRegressor', 'ExtraTreesRegressor', 'XGBoost', 'LightGBM']#, 'GaussianProcess']\n",
    "candi_data_file = 'Chen_candidate/combined_testset.csv'\n",
    "BO = BayesianOptimization(data_file, target_props, model_list=model_list, acq_method='ucb', candidate_file=candi_data_file)#, select_region=[[2.200],[2.201]])\n",
    "BO.optimize(batch_size=200, n_bootstrap_sample_nums=81, sampling_method='gaussian', num_candidate=100, n_samples=1000, iterations=5, hpar=0.1, if_train=False)\n",
    "\n",
    "testset_path = 'Chen_candidate/suggested_samples_final.csv'\n",
    "os.popen(f'mv suggested_samples_original.csv {testset_path}')\n",
    "os.popen(f'rm suggested_samples_indexes.csv suggested_samples.csv')\n",
    "\n",
    "train_df = pd.read_csv(data_file)\n",
    "feature_columns = train_df.columns.drop('density')\n",
    "test_df = pd.read_csv(testset_path)\n",
    "test_df.columns = feature_columns\n",
    "test_df['density'] = None\n",
    "\n",
    "test_df.to_csv(testset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce4688-5751-4d7b-b6a8-f3ac9b077958",
   "metadata": {},
   "source": [
    "# Other side functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abada4-d3bd-4be6-b68f-34c4c96f75f9",
   "metadata": {},
   "source": [
    "### check model, plotting and predicting candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cecf2-dc0d-46f3-9a9b-53a1f55b5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "scaler_method='standard'\n",
    "io_manager = IOManager(method=scaler_method)\n",
    "X_train, y_train = io_manager.read_data(data_file, target_props=BO.target_props, feature_props=BO.feature_props, handle_null=True, drop_non_numeric=True)\n",
    "# X_candi = io_manager.read_candidate_data(candi_data_file, target_props=BO.target_props, feature_props=BO.feature_props, drop_non_numeric=True)\n",
    "# X_scaled, y_scaled, candidate_X_scaled = io_manager.standardize_data(X_train, y_train, X_candi)\n",
    "X_candi, y_candi = io_manager.read_data(candi_data_file, target_props=BO.target_props, feature_props=BO.feature_props, handle_null=True, drop_non_numeric=True)\n",
    "X_scaled, y_scaled, candidate_X_scaled, candidate_y_scaled = io_manager.standardize_data(X_train, y_train, X_candi, y_candi)\n",
    "# candidate_X_scaled, candidate_y_scaled = io_manager.standardize_data(X_candi, y_candi)\n",
    "# X_scaled, y_scaled = io_manager.standardize_data(X_train, y_train)\n",
    "\n",
    "model_path = f'model_weights'\n",
    "models = [i for i in os.listdir(f'{model_path}') if i.endswith('.pkl')]\n",
    "\n",
    "for m in models:\n",
    "    with open(f'{model_path}/{m}', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    if 'bootstrap' in m:\n",
    "        bs_models = model['models']\n",
    "        pred_y = np.mean([m.predict(candidate_X_scaled) for m in bs_models], axis=0)\n",
    "    else:\n",
    "        bs_models = model['stacking_model']\n",
    "        pred_y = bs_models.predict(candidate_X_scaled)\n",
    "\n",
    "    # print(pred_y.reshape(-1,1))\n",
    "    # io_manager.scaler_y.fit(y_train)\n",
    "    y_prediction = io_manager.scaler_y.inverse_transform(pred_y.reshape(-1,1))\n",
    "\n",
    "    print(y_prediction)\n",
    "\n",
    "    r2 = r2_score(y_prediction, y_candi)\n",
    "    mse = mean_squared_error(y_prediction, y_candi)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(y_candi, y_prediction.reshape(-1))\n",
    "    miny, maxy = min(np.min(y_prediction), np.min(y_candi)), max(np.max(y_prediction), np.max(y_candi))\n",
    "    X_axis = np.linspace(miny, maxy, num=50)\n",
    "    ax.plot(X_axis, X_axis, c='r')\n",
    "    ax.set_title(f'Models {m}', size=20)\n",
    "    ax.text(miny+0.05, maxy-0.1, s=f\"MSE: {round(mse,3)}\", fontsize=18, color='black')\n",
    "    ax.text(miny+0.05, maxy-0.2, s=f\"R2: {round(r2,3)}\", fontsize=18, color='black')\n",
    "    plt.savefig(f'{model_path}/{m[:-4]}.png', bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    csv_file = pd.read_csv(candi_data_file)\n",
    "    csv_file['density'] = y_prediction\n",
    "    csv_file.to_csv(f'{model_path}/{m[:-4]}.csv', index=False)\n",
    "    \n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#     ax.scatter(pred_y, y_scaled.reshape(-1))\n",
    "#     ax.set_title(f'Models {m}', size=20)\n",
    "#     ax.set_xlim(-3, 3)\n",
    "#     ax.set_ylim(-3, 3)\n",
    "#     plt.savefig(f'{root}/{data_path}/{m[:-4]}.png', bbox_inches='tight', dpi=150)\n",
    "#     plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb62ff-98a5-4983-890a-f851701c6230",
   "metadata": {},
   "source": [
    "### select specific number of element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df642a-f6fe-4d7a-8c4f-dfba87f322d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "for num in range(1,21):\n",
    "    df = pd.read_csv(f'Chen_candidate/sep_candidate_{num}.csv')\n",
    "    \n",
    "    element_pattern = re.compile(r'([A-Z][a-z]?)(\\d*\\.?\\d*)')\n",
    "    \n",
    "    def get_elements(composition):\n",
    "        elements = element_pattern.findall(composition)\n",
    "        element_dict = {elem: float(count) if count else 1.0 for elem, count in elements}\n",
    "        return element_dict\n",
    "    \n",
    "    def filter_compositions(row):\n",
    "        elements = get_elements(row['composition'])\n",
    "        elements_filtered = {k: v for k, v in elements.items() if k not in 'Cl'}\n",
    "        if len(elements_filtered) == 5:\n",
    "            if 'Zr' not in elements_filtered.keys() and 'Zn' not in elements_filtered.keys():\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    df_filtered = df[df.apply(filter_compositions, axis=1)]\n",
    "    df_filtered.to_csv(f'Chen_candidate/filtered_5_compositions_{num}.csv', index=False)\n",
    "    \n",
    "    print(f\"Finish filtering，saveing results to filtered_compositions_{num}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33f7be-1568-4cac-9251-18cf4bb8301a",
   "metadata": {},
   "source": [
    "### position transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b9902-5121-4fed-8f70-be86a237df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "element_to_cl_ratio = {\n",
    "    'Li': 1, 'Na': 1, 'K': 1, 'Rb': 1, 'Cs': 1,\n",
    "    'Mg': 2, 'Ca': 2, 'Sr': 2, 'Ba': 2, 'Zn': 2,\n",
    "    'Zr': 4\n",
    "}\n",
    "\n",
    "element_pattern = re.compile(r'([A-Z][a-z]?)(\\d*\\.?\\d*)')\n",
    "\n",
    "def get_elements(composition):\n",
    "    elements = element_pattern.findall(composition)\n",
    "    element_dict = {elem: float(count) if count else 1.0 for elem, count in elements}\n",
    "    return element_dict\n",
    "\n",
    "def round_to_grid(value, grid=0.05):\n",
    "    return round(value / grid) * grid\n",
    "\n",
    "def normalize_composition(composition, grid=0.05):\n",
    "    elements = get_elements(composition)\n",
    "    \n",
    "    non_cl_elements = {k: v for k, v in elements.items() if k != 'Cl'}\n",
    "    total_non_cl = sum(non_cl_elements.values())\n",
    "\n",
    "    normalized_elements = {k: v / total_non_cl for k, v in non_cl_elements.items()}\n",
    "\n",
    "    total_cl = sum(normalized_elements[k] * element_to_cl_ratio[k] for k in normalized_elements)\n",
    "\n",
    "    normalized_elements = {k: round_to_grid(v, grid) for k, v in normalized_elements.items()}\n",
    "    total_cl_rounded = round_to_grid(total_cl, grid)\n",
    "    \n",
    "    # normalized_composition = ''.join([f\"{k}{v:.2f}\" for k, v in normalized_elements.items()])\n",
    "    # normalized_composition += f\"Cl{total_cl_rounded:.2f}\"\n",
    "\n",
    "    normalized_composition = ''.join([f\"{k}{round(v, 2)}\" for k, v in normalized_elements.items()])\n",
    "    normalized_composition += f\"Cl{round(total_cl_rounded, 3)}\"\n",
    "    \n",
    "    return normalized_composition\n",
    "\n",
    "### Tests\n",
    "file = 'Dataprocessing/collection_4.csv'\n",
    "data = pd.read_csv(file)\n",
    "compositions = data['composition']\n",
    "normalized_compositions = [normalize_composition(composition) for composition in compositions]\n",
    "data['norm_composition'] = normalized_compositions\n",
    "data.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec17f0-bcb4-4efc-b806-058d905b24f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bo_project",
   "language": "python",
   "name": "bo_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
